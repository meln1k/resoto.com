<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Resoto by Some Engineering Blog</title>
        <link>https://resoto.com/news</link>
        <description>Resoto by Some Engineering Blog</description>
        <lastBuildDate>Tue, 18 Jan 2022 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[v2.0.0a10]]></title>
            <link>https://resoto.com/news/2022/01/18/v2.0.0a10</link>
            <guid>/2022/01/18/v2.0.0a10</guid>
            <pubDate>Tue, 18 Jan 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[This release is exciting since it is the first one featuring our new product name, Resoto.]]></description>
            <content:encoded><![CDATA[<p>This release is exciting since it is the first one featuring our new product name, <em>Resoto</em>.</p><p>Cloudkeeper is no more, along with all <code>ck</code> naming.</p><p>The components that make up Resoto are now called Resoto Core (<code>resotocore</code>), Resoto Shell (<code>resh</code>), Resoto Worker (<code>resotoworker</code>), and Resoto Metrics (<code>resotometrics</code>).</p><p>:::note
The Docker image name has also changed to <code>somecr.io/someengineering/resoto</code>.
:::</p><p>Besides the naming change, a lot of features have been implemented. Here is a summary of the most important ones:</p><h2>Query and CLI Interpretation Relative to the Reported Section</h2><p>Resoto has the concept of a property path. A deeply nested json object can be navigated by defining a path to it.</p><p>:::tip Example
<code>reported.name</code> means the <code>name</code> property inside the <code>reported</code> object.
:::</p><p>Until now, we interpreted all paths from the object root. But while this might be the natural approach, we realized that most of the properties to deal with are in the reported section.</p><p>To make query and commands most effective, Resoto will now interpret any path relative to the reported section.</p><p>A property path can be prefixed with a <code>/</code> to mark it as an absolute path. This allows accessing properties not in the reported section.</p><p>The described behavior is used in queries and all other commands you use on the command line.</p><p>:::tip Example
A volume (e.g. an <a href="https://resoto.com/docs/reference/resources/aws#aws_ec2_volume">aws_ec2_volume</a>) has a <code>name</code> and
<code>volume_size</code> property in the reported section, that can be accessed directly.</p><pre><code class="language-bash">$&gt; query is(volume) and name=~pvc | list name, volume_size
</code></pre><p>An absolute path is now required to access properties not in the reported section.
Assuming we want to query data in the metadata section, the path has to be prefixed with a <code>/</code>.</p><pre><code class="language-bash">$&gt; query is(volume) and /metadata.cleaned==false | list /metadata.protected
</code></pre><p>:::</p><p>This new way of path resolution allowed us to remove several CLI commands like <code>reported</code>, <code>desired</code> and <code>metadata</code>.</p><h2>Ancestor merges are now happening automagically</h2><p>References to ancestors are available in their section <code>/ancestors</code>.</p><p>As with previous versions, id and name of cloud, account, region and zone are added and displayed automatically for every resource. It is now possible to use any other property besides id and name.</p><p>:::tip Example</p><pre><code class="language-bash">$&gt; query is(volume) and /ancestors.region.metadata.intent=test
</code></pre><p>:::</p><h2>More Powerful Formats</h2><p>The format CLI command now understands some predefined formats, that can be enabled via command line flags.</p><p>The following formats are available out of the box:</p><ul><li><code>--graphml</code> <a href="http://graphml.graphdrawing.org">graphml</a> is a widely used format to export graph data.</li><li><code>--cytoscape</code> <a href="https://js.cytoscape.org">cytoscape</a> vendor specific format.</li><li><code>--dot</code> export the data in <a href="https://graphviz.org">graphviz</a> dot format.</li><li><code>--json</code> creates a JSON string for any node in the system put into a json array.</li><li><code>--ndjson</code> new line delimited JSON, so every node is a one line JSON document.</li><li><code>--text</code> creates a text representation of every node.</li></ul><p>:::tip Example
Here, we export the whole graph including all edges to a file named <code>graph.json</code>:</p><pre><code class="language-bash">$&gt; query --include-edges is(graph_root) -[0:]-&gt;  | format --json | write graph.json
</code></pre><p>And here, we export a query result in dot format.</p><pre><code class="language-bash">$&gt; query --include-edges is(aws_ec2_instance) &lt;-[0:1]- | format --dot | write edges.dot
</code></pre><p>:::</p><p>If you have <a href="https://graphviz.org">graphviz</a> installed, you can turn this file into a diagram using this command: <code>shell&gt; sfdp -Tpng -o edges.png edges.dot</code>.</p><h2>Improved Jobs Command</h2><p>There is now a single command to create, manipulate and test jobs using the <code>jobs</code> command.</p><p>Please see the help page of jobs <code>$&gt; help jobs</code> to get more details.</p><p>We removed <code>add_job</code>, <code>delete_job</code> and <code>tasks</code> command.</p><h2><code>http</code> Command</h2><p>We introduced an <code>http</code> and <code>https</code> command in the CLI that allows to do a REST call to other systems in case of predefined scenarios in Resoto.</p><p>:::tip Example
Assume we want to call an internal gatekeeper in case we find compute instances that are tagged with <code>load-test</code> and are older than 24 hours.</p><pre><code class="language-bash">$&gt; query is(instance) and tags.intent=&quot;load-test&quot; and age&gt;24h | chunk 100 | format --json |  http gatekeeper/handle_expired
</code></pre><p>The results of this query are chunked, every chunk of 100 elements is json formatted and sent to the gatekeeper.</p><p>It would be possible to make this a recurrent task by creating a job.
:::</p><h2>Query Cost Analysis</h2><p>Writing queries can be a complex task.
To give some guidance related to query runtime performance, we introduced the option to analyze a query.
The query command now has an <code>--explain</code> flag, that gives insights into the query performance.</p><p>:::tip Example
We use the previous query as example to show a possible result. (Please note that the runtime characteristics are highly dependent on the underlying graph data.)</p><pre><code class="language-bash">$&gt; query --explain is(instance) and tags.intent=&quot;load-test&quot; and age&gt;24h
available_nr_items: 142670
estimated_cost: 4891
estimated_nr_items: 4641
full_collection_scan: false
rating: simple
</code></pre><p>:::</p><p>The final rating is the most interesting one, where Resoto defines a query as simple, complex or bad. A query that is considered simple should be fast and resource friendly.</p><h2>Other improvements</h2><ul><li>Resoto is now using <a href="https://app.codecov.io/gh/someengineering/resoto">Codecov</a> for coverage reports.</li><li>This release ships with APIs and functionality to get a complete TLS secured setup.
This is still work in progress so all communication is plain http.
We will have a complete secure setup when the final 2.0 release is shipped.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[v2.0.0a9]]></title>
            <link>https://resoto.com/news/2021/12/09/v2.0.0a9</link>
            <guid>/2021/12/09/v2.0.0a9</guid>
            <pubDate>Thu, 09 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[We are very happy to announce that we are now another small step closer to a stable 2.0 release!]]></description>
            <content:encoded><![CDATA[<p>We are very happy to announce that we are now another small step closer to a stable 2.0 release!</p><p>Here are some highlights from this release:</p><h2>The UI is now shipped as part of every release</h2><p>This is the first version that ships with our gorgeous UI.</p><p>Please try it out by downloading the latest version and navigating to <code>https://path.to.cloudkeeper:8900/ui</code> in your browser.</p><p>The main graph view has been upgraded from 2D to 3D, and shows Treemap charts (<a href="https://github.com/someengineering/resoto/pull/457">#457</a>)!</p><h2>We now have a helm chart</h2><p>Thanks to a contribution from <a href="https://github.com/yuval-k">@yuval-k</a>, we now have a Helm chart (<a href="https://github.com/someengineering/resoto/pull/428">#428</a>)!</p><p>With this chart, it is easier to deploy Cloudkeeper in Kubernetes.</p><p>Try it out yourself by following the <a href="https://docs.some.engineering/getting_started/setup_kubernetes.html">Kubernetes setup tutorial in our documentation</a>.</p><h2>All cleanup plugins are now available</h2><p>We needed to migrate all cleanup plugins to the 2.0 infrastructure.</p><p>With this release, all cleanup plugins have been ported (<a href="https://github.com/someengineering/resoto/pull/422">#422</a>) and (<a href="https://github.com/someengineering/resoto/pull/439">#439</a>).</p><h2>Analytics sensors were added</h2><p>We believe that it is important to know how Cloudkeeper is used, and thus how we can improve.</p><p>As such, we have added analytics to our codebase.</p><p>The data that is anonymized and purely focused on providing product insights.</p><p>It is possible to opt out of sending this data by specifying the command line flag <code>--analytics-opt-out</code>.</p><h2>Query language improvements</h2><p>There are several significant improvements in this area.</p><p>It is now possible to define sub-queries (<a href="https://github.com/someengineering/resoto/pull/412">#412</a>) which allow merging nodes with other nodes in the graph.</p><p>Additionally, the first step toward a full-featured query template engine has been implemented in <a href="https://github.com/someengineering/resoto/pull/431">#431</a>.</p><p>This feature allows defining queries as a template and reusing those templates in other queries, greatly simplifying more complex queries.</p><h2>Other improvements</h2><ul><li><code>[ckcore]</code> In the CLI the default output style is now the list style. Every node is printed as one line. To show all available data as yaml node, we introduced the dump command. (<a href="https://github.com/someengineering/resoto/pull/425">#425</a>)</li><li><code>[plugin/gcp]</code> only collect referenced type and service resources, so the graph only contains used resources. (<a href="https://github.com/someengineering/resoto/pull/430">#430</a>)</li><li><code>[ckcore]</code> Add support for array modifiers <code>all, any, none</code>. Example: <code>reported.array all &gt; 3</code>, which selects all nodes where the property <code>reported.array</code> points to an array of integers and all integers in that array are bigger than 3. (<a href="https://github.com/someengineering/resoto/pull/427">#427</a>)</li><li><code>[ckcore]</code> arangodb 3.8.2 or later is now the minimum required version to run resoto. (<a href="https://github.com/someengineering/resoto/pull/445">#445</a>)</li><li><code>[ckcore]</code> <code>tag</code> command can be backgrounded. (<a href="https://github.com/someengineering/resoto/pull/437">#437</a>)</li><li><code>[ckcore]</code> <code>is()</code> now also supports multiple kinds, with an or meaning. Example `is(volume, instance) (<a href="https://github.com/someengineering/resoto/pull/432">#432</a>)</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[v2.0.0a3]]></title>
            <link>https://resoto.com/news/2021/10/04/v2.0.0a3</link>
            <guid>/2021/10/04/v2.0.0a3</guid>
            <pubDate>Mon, 04 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[July 1st, 2021 was a big day for Cloudkeeper. We started a new company to focus 100% on building out Cloudkeeper. The new company's name is "Some Engineering Inc.", and going forward we are the maintainers of the open-source project.]]></description>
            <content:encoded><![CDATA[<p>July 1st, 2021 was a big day for Cloudkeeper. We started a new company to focus 100% on building out Cloudkeeper. The new company&#x27;s name is &quot;Some Engineering Inc.&quot;, and going forward we are the maintainers of the open-source project.</p><p>Cloudkeeper started as an internal project in late 2019 at D2iQ, the Enterprise Kubernetes Platform, to solve the problem of &quot;better housekeeping&quot; for D2iQ&#x27;s cloud accounts. Find leaky resources, manage quota limits, detect drift, clean up and reduce cost. It&#x27;s the stuff no engineer wants to deal with. Our co-founder Lukas was a Site Reliability Engineer at D2iQ, and he was in charge of the cleanup project. He needed a tool to give him the big picture of all cloud resources running. And then use that inventory to identify and clean up the resources not in use anymore.</p><p>Fast forward to today, and Cloudkeeper has been in production at D2iQ for almost two years. It has grown &quot;organically&#x27;&#x27; to its current functionality. The SRE team at D2iQ knows all the ins and out of the project. But for somebody new to the project, the bar to start was quite high.</p><p>For the past two months, we&#x27;ve been busy changing that. And the result is release 2.0.0a3. With this new release, we took out the friction to get Cloudkeeper running in a cloud account. At the same time, we also added a lot of new functionality that makes Cloudkeeper more useful.</p><p>We also wrote a lot of new documentation to make it easier for a new user to start with Cloudkeeper. It&#x27;s far from done yet, and we&#x27;re adding new sections every day.</p><p>But let&#x27;s dive into the updates!</p><h2>Architecture: From Monolith to Distributed System</h2><p>We rebuilt Cloudkeeper from the ground up to make it extensible and scalable. The first version of Cloudkeeper was monolithic with a single binary and ran in-memory locally on a laptop. We broke down the single binary and now provide four different binaries:</p><ol><li><code>ckcore</code> maintains cloud-agnostic data in a graph</li><li><code>ckworker</code> executes infrastructure-specific plug-ins</li><li><code>cksh</code> starts the Cloudkeeper shell</li><li><code>ckmetrics</code> calculates metrics in Prometheus format</li></ol><p>The benefit of this approach is that it scales. The length of a full Cloudkeeper run is subject to the number of accounts in a cloud. If you have hundreds or even thousands of accounts—it just takes longer to collect all resources. With this new architecture, you can now add more <code>ckworker</code> for faster processing.</p><p>This distributed architecture is also more flexible. A clear and simple API helps deal with cloud-specific data. Right now we support AWS, but eventually, we will also build support for GCP, Azure, Alicloud, etc. Different workers give you the freedom of choice to allocate workers, with different configurations. For example, you can have different workers for different clouds, and split the workloads that way. Or, you assign a worker for each individual login. In other words, you can run workers in whatever combination, to reflect e.g. your multi-cloud, geo, account or login structure of your cloud.</p><h3>Components</h3><p>A bit more detail on the four components of the architecture.</p><ul><li><code>ckcore</code>, a.k.a. &quot;the core,&quot; maintains the graph. Data collection happens via <code>ckworker</code>. The workers push data into <code>ckcore</code>, after the core has told the workers to start collecting data. In the graph, nodes are individual resources, edges are logical dependencies. Cloudkeeper stores a resource&#x27;s attributes in the node. These attributes are the basis for the dependencies that Cloudkeeper creates.
We built <code>ckcore</code> with a scheduler and a message bus. The message bus has topics and queues. The scheduler runs internally in the core, by default the collect event gets triggered once per hour. A user can however define their own schedule by using the Cloudkeeper shell <code>cksh</code>.</li><li><code>ckworker</code> does all the collection and cleanup work in Cloudkeeper. It waits for instructions from <code>ckcore</code> over a WebSocket connection. By default <code>ckworker</code> subscribes to collect, clean up and tag tasks.</li><li><code>cksh</code> is our command-line interface, aka &quot;the shell.&quot; The CLI allows you to execute a variety of commands (see query language) to explore the graph, find resources of interest, mark them for cleanup, fix their tagging, aggregate over their metadata to create metrics and format the output for use in a 3rd party script or system.</li><li><code>ckmetrics</code> takes graph data from <code>ckcore</code> and runs aggregation functions on it. The aggregated metrics are then exposed in a Prometheus-compatible format for consumption in other services. For example, D2iQ uses Grafana dashboards to visualize infrastructure metrics for Engineering, Finance and the CEO.</li></ul><h2>Graph Storage: From In-Memory to On-Disk Persistence</h2><p>One of the biggest asks by early users has been data persistence. With the new version of Cloudkeeper, we migrated from a locally maintained in-memory graph to a backend where we now persist the graph after each collect run. Under the hood, we use ArangoDB for that.</p><p>Data persistence has three major advantages.</p><ul><li>It is the foundation to create a history and different versions of the graph. In the past, with the in-memory only version, a restart would make Cloudkeeper lose all history. Right now Cloudkeeper persists the latest collected snapshot, and we have history on our roadmap.</li><li>By persisting the graph we can also provide an audit trail of all changes. Cloudkeeper attaches the changelog to the node that represents the resource. In the previous version, you lost the changelog associated with each node once a new collect run started. Once we keep a history of snapshots, we can also provide a history of changes. Particularly users in the financial services industry have asked for that capability.</li><li>The size of the data set Cloudkeeper collects and stores is not limited anymore by available memory. It&#x27;s essentially unlimited now by adding more storage at the database layer. This means Cloudkeeper can work with the largest cloud and multi-cloud infrastructure(s).</li></ul><p>Data persistence also means better collaboration, because two people can now look at the same version of the graph. Previously, their local versions of the graph would be different from each other, simply because the information was collected at different points in time.</p><p>We also switched to incremental updates. Every time a collector runs, it collects all resources in your entire cloud and sends the information to the core. The old version would push the entire new graph, which is fine for an in-memory store. But now with data persistence and disk, we wanted to optimize for fewer writes. In this new version, the core compares what a new collect run delivered with the current state of the graph, and only stores the delta between the two.</p><h2>Query Language: From Constrained to Flexible</h2><p>In the previous version of Cloudkeeper, plug-ins delivered much of the rich functionality. The issue with that approach is that for each new use case, you need to create a new plug-in. Plug-ins are useful, but they require writing code and deploying the change. It also means the number of plug-ins keeps going up as you add more use cases, and it gets confusing pretty fast.</p><p>Instead, we evolved the query language to include more commands and richer query syntax. The benefit for the users is that you don&#x27;t have to write and maintain yet another plug-in—you just write a single query.</p><p>A really nice new functionality of the query language is graph traversal over multiple nodes. In the old version of Cloudkeeper, you could only match and filter by attributes for an individual resource. Now, with graph traversal, you can also filter and match based on the state of all predecessor and successor nodes. This is a super powerful capability to navigate the graph, express complex conditions across multiple resources in a single query, and take action on resources that match those conditions.</p><h3>Example</h3><p>Let&#x27;s illustrate this with a specific use case—cleaning up unused application load balancers (&quot;ALB&quot;) in AWS. Load balancers distribute incoming application traffic across multiple targets, such as EC2 instances, which are attached to multiple target groups. In short, <em>load balancer → target group(s) → compute instances</em>.</p><p>To determine if a load balancer is still in use or not, you have to know if there are no more backend instances, or if they are still connected but terminated (which is particular to AWS). This may seem easy, but in a multi-account structure—for every account you would have to look for load balancers in every region, understand which ones have target groups, which target groups have instances, and understand the state of each instance. If there is an instance still running, we can&#x27;t delete the target group of the load balancer, because it might still be in use. Going through that decision loop for every load balancer is impossible without automation. Unless you want to spend your time clicking through the account structure of your AWS console.</p><p>Why would this matter? Load balancers are not that expensive. But companies usually have thousands of them—it adds up, and there are quotas. The default is 50 ALBs and 100 ALB target groups per region. You can increase the quota by 10x or even 100x. But when you leak resources, it&#x27;s easy to hit even a high limit like 5,000.</p><p>And so going through that decision loop to find unused ALBs without any automation is almost impossible. With graph traversal, we can write a query that finds unused load balancers, by determining if the target groups are empty, if the instances are not running anymore, or if they are connected but terminated. We define &quot;unused&quot; as &quot;older than 7 days&quot; <code>ctime &lt; -7d</code> and &quot;no backends attached.&quot;</p><pre><code class="language-bash">is(aws_alb) and ctime &lt; -7d with(empty, &lt;-- is(aws_alb_target_group)
  and target_type = instance and ctime &lt; -7d with(empty, &lt;-- is(aws_ec2_instance)
  and instance_status != terminated)) &lt;-[0:1]- is(aws_alb_target_group) or is(aws_alb)
</code></pre><p>That&#x27;s it! This query will generate a list of all orphaned load balancers that are candidates for clean-up. To actually clean up, we only need to add a <code>| clean</code> command at the end of the query.</p><pre><code class="language-bash">is(aws_alb) and ctime &lt; -7d with(empty, &lt;-- is(aws_alb_target_group)
  and target_type = instance and ctime &lt; -7d with(empty, &lt;-- is(aws_ec2_instance)
  and instance_status != terminated)) &lt;-[0:1]- is(aws_alb_target_group) or is(aws_alb) | clean
</code></pre><h2>Metrics: From Hard-Coded to Query-Based</h2><p>In the old version of Cloudkeeper, the metrics for each resource were hard-coded. At D2iQ, you would literally have to ask Lukas to write the code for a new metric. Obviously, that&#x27;s not a great long-term solution. The new query language can now do selection and aggregation, and a user can write queries that generate custom metrics.</p><p>The benefit is that each audience (engineering, product, finance, etc. ) can create the exact metric they need. Let&#x27;s pick an example to illustrate how to write a query that generates metrics.</p><p>Assume a CFO wants to know the cost of all AWS compute instances that are running, in nearn The query below calculates a total hourly on-demand cost estimate for all EC2 instances running in all AWS accounts, and aggregates the result by account, region and instance type.</p><pre><code class="language-bash"> query is(aws_ec2_instance) and reported.instance_status = running |
    merge_ancestors
      account,region,instance_type |
    aggregate
      reported.account.name as account,
      reported.region.name as region,
      reported.instance_type.name as type :
    sum(reported.instance_type.ondemand_cost) as instances_hourly_cost_estimate
</code></pre><p><code>instance_type</code> is a resource in the Cloudkeeper graph. The node for the resource contains a field for the on-demand cost. Cloudkeeper fetches the data for that field from the AWS Pricing API during each collection run.</p><p>The query then generates a new metric <code>instances_hourly_cost_estimate</code>—a total hourly cost estimate, broken down by account, region and instance type. It&#x27;s a simple way to understand which AWS accounts and the teams responsible for the accounts drive compute cost. And the nice thing is that Finance doesn&#x27;t have to bother engineering to get these metrics. They can just run the queries themselves in the Cloudkeeper CLI.</p><p>Writing queries may not be everyone&#x27;s thing though. For those users, we also maintain several pre-configured metrics per resource in Cloudkeeper.</p><p>These pre-configured metrics are running as queries in <code>ckmetrics</code>. <code>ckmetrics</code> connects to the core, runs the queries and recalculates the metrics automatically every time something has changed in the graph, e.g. after a collect or a clean-up. The results are cached in <code>ckmetrics</code> and exported to Prometheus where they can be queried via PromQL. From there, you can send them to any visualization tool that understands the prometheus format, such as Grafana. Future versions of <code>ckmetrics</code> will allow a user to edit the pre-defined metrics as well as define their own.</p><h2>Command-Line Interface: From Local to Remote Execution</h2><p>The old CLI ran locally on a user&#x27;s desktop. That implied that two different users would never look at the same version of a graph, because it was their own &quot;local&quot; version that Cloudkeeper had generated at a specific point in time.</p><p>The new CLI executes commands remotely in the core. That means everyone now looks at the same version of a graph, which opens up new collaboration use cases.</p><h2>Workflows: From Hard-Coded to Event-Based</h2><p>Currently we support three different workflows—collect, clean up and metrics. Workflows consist of steps that perform a specific action.</p><p>In the old Cloudkeeper, the execution order of these workflows and their steps was hard-coded. Collect, clean-up, metrics. If you wanted to update your metrics—you had to execute the whole thing again. You couldn&#x27;t flexibly re-arrange the steps, skip a step, or call one on-demand.</p><p>Now, you can schedule and execute workflows in whatever scope and order you want. For example, once Cloudkeeper has collected and generated a new graph, you can look at the graph, write a query that flags certain resources for clean-up, and trigger the clean-up.</p><p>Workflows are an area that we&#x27;re investing strongly in. If you have ideas, please let us know!</p><h2>What&#x27;s Next?</h2><p>This current release makes it much easier to use Cloudkeeper to keep your cloud clean of drift. We made it easier and more intuitive for first-time users to start with Cloudkeeper. And we have a lot more ideas to keep going in that direction. For example, the next release will have a built-in library of useful query templates to give users a jump start. We&#x27;re also working on authentication, authorization and encrypted communication.</p><p>Meanwhile, please let us know what&#x27;s important for you as we continue building out Cloudkeeper. We also offer custom onboarding sessions on <a href="https://discord.gg/someengineering">Discord</a>!</p>]]></content:encoded>
        </item>
    </channel>
</rss>